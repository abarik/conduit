{"uid":"f709452ef115bb9d","name":"test_01","fullName":"test_conduit.tests.test_cases_11.TestConduit#test_01","historyId":"eb2901fec36f57beba43c80f44b9a7ed","time":{"start":1654520411063,"stop":1654520411063,"duration":0},"status":"broken","statusMessage":"AttributeError: 'TestConduit' object has no attribute 'mp'","statusTrace":"self = <test_conduit.tests.test_cases_11.TestConduit object at 0x7ff843caa230>\n\n    def setup(self):\n        try:\n>           self.data_for_test = DataForTest()\n\ntest_conduit/tests/test_cases_11.py:15: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <test_conduit.config.data_for_test.DataForTest object at 0x7ff843caa380>\n\n    def __init__(self):\n>       DataForTest.ARTICLE_TEST_DATA = pd.read_csv(os.path.realpath('test_conduit/config/' + DataForTest.FILE_ARTICLE_DATA),\n                                                    delimiter='\\t')\n\ntest_conduit/config/data_for_test.py:36: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nargs = ('/home/runner/work/conduit/conduit/test_conduit/config/article_test_data.txt',)\nkwargs = {'delimiter': '\\t'}\narguments = \" except for the argument 'filepath_or_buffer'\"\n\n    @wraps(func)\n    def wrapper(*args, **kwargs):\n        arguments = _format_argument_list(allow_args)\n        if len(args) > num_allow_args:\n            warnings.warn(\n                msg.format(arguments=arguments),\n                FutureWarning,\n                stacklevel=stacklevel,\n            )\n>       return func(*args, **kwargs)\n\n/opt/hostedtoolcache/Python/3.10.4/x64/lib/python3.10/site-packages/pandas/util/_decorators.py:311: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nfilepath_or_buffer = '/home/runner/work/conduit/conduit/test_conduit/config/article_test_data.txt'\nsep = <no_default>, delimiter = '\\t', header = 'infer', names = <no_default>\nindex_col = None, usecols = None, squeeze = None, prefix = <no_default>\nmangle_dupe_cols = True, dtype = None, engine = None, converters = None\ntrue_values = None, false_values = None, skipinitialspace = False\nskiprows = None, skipfooter = 0, nrows = None, na_values = None\nkeep_default_na = True, na_filter = True, verbose = False\nskip_blank_lines = True, parse_dates = None, infer_datetime_format = False\nkeep_date_col = False, date_parser = None, dayfirst = False, cache_dates = True\niterator = False, chunksize = None, compression = 'infer', thousands = None\ndecimal = '.', lineterminator = None, quotechar = '\"', quoting = 0\ndoublequote = True, escapechar = None, comment = None, encoding = None\nencoding_errors = 'strict', dialect = None, error_bad_lines = None\nwarn_bad_lines = None, on_bad_lines = None, delim_whitespace = False\nlow_memory = True, memory_map = False, float_precision = None\nstorage_options = None\n\n    @deprecate_nonkeyword_arguments(\n        version=None, allowed_args=[\"filepath_or_buffer\"], stacklevel=3\n    )\n    @Appender(\n        _doc_read_csv_and_table.format(\n            func_name=\"read_csv\",\n            summary=\"Read a comma-separated values (csv) file into DataFrame.\",\n            _default_sep=\"','\",\n            storage_options=_shared_docs[\"storage_options\"],\n            decompression_options=_shared_docs[\"decompression_options\"],\n        )\n    )\n    def read_csv(\n        filepath_or_buffer: FilePath | ReadCsvBuffer[bytes] | ReadCsvBuffer[str],\n        sep=lib.no_default,\n        delimiter=None,\n        # Column and Index Locations and Names\n        header=\"infer\",\n        names=lib.no_default,\n        index_col=None,\n        usecols=None,\n        squeeze=None,\n        prefix=lib.no_default,\n        mangle_dupe_cols=True,\n        # General Parsing Configuration\n        dtype: DtypeArg | None = None,\n        engine: CSVEngine | None = None,\n        converters=None,\n        true_values=None,\n        false_values=None,\n        skipinitialspace=False,\n        skiprows=None,\n        skipfooter=0,\n        nrows=None,\n        # NA and Missing Data Handling\n        na_values=None,\n        keep_default_na=True,\n        na_filter=True,\n        verbose=False,\n        skip_blank_lines=True,\n        # Datetime Handling\n        parse_dates=None,\n        infer_datetime_format=False,\n        keep_date_col=False,\n        date_parser=None,\n        dayfirst=False,\n        cache_dates=True,\n        # Iteration\n        iterator=False,\n        chunksize=None,\n        # Quoting, Compression, and File Format\n        compression: CompressionOptions = \"infer\",\n        thousands=None,\n        decimal: str = \".\",\n        lineterminator=None,\n        quotechar='\"',\n        quoting=csv.QUOTE_MINIMAL,\n        doublequote=True,\n        escapechar=None,\n        comment=None,\n        encoding=None,\n        encoding_errors: str | None = \"strict\",\n        dialect=None,\n        # Error Handling\n        error_bad_lines=None,\n        warn_bad_lines=None,\n        # TODO(2.0): set on_bad_lines to \"error\".\n        # See _refine_defaults_read comment for why we do this.\n        on_bad_lines=None,\n        # Internal\n        delim_whitespace=False,\n        low_memory=_c_parser_defaults[\"low_memory\"],\n        memory_map=False,\n        float_precision=None,\n        storage_options: StorageOptions = None,\n    ):\n        # locals() should never be modified\n        kwds = locals().copy()\n        del kwds[\"filepath_or_buffer\"]\n        del kwds[\"sep\"]\n    \n        kwds_defaults = _refine_defaults_read(\n            dialect,\n            delimiter,\n            delim_whitespace,\n            engine,\n            sep,\n            error_bad_lines,\n            warn_bad_lines,\n            on_bad_lines,\n            names,\n            prefix,\n            defaults={\"delimiter\": \",\"},\n        )\n        kwds.update(kwds_defaults)\n    \n>       return _read(filepath_or_buffer, kwds)\n\n/opt/hostedtoolcache/Python/3.10.4/x64/lib/python3.10/site-packages/pandas/io/parsers/readers.py:680: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nfilepath_or_buffer = '/home/runner/work/conduit/conduit/test_conduit/config/article_test_data.txt'\nkwds = {'cache_dates': True, 'chunksize': None, 'comment': None, 'compression': 'infer', ...}\n\n    def _read(\n        filepath_or_buffer: FilePath | ReadCsvBuffer[bytes] | ReadCsvBuffer[str], kwds\n    ):\n        \"\"\"Generic reader of line files.\"\"\"\n        # if we pass a date_parser and parse_dates=False, we should not parse the\n        # dates GH#44366\n        if (\n            kwds.get(\"date_parser\", None) is not None\n            and kwds.get(\"parse_dates\", None) is None\n        ):\n            kwds[\"parse_dates\"] = True\n        elif kwds.get(\"parse_dates\", None) is None:\n            kwds[\"parse_dates\"] = False\n    \n        # Extract some of the arguments (pass chunksize on).\n        iterator = kwds.get(\"iterator\", False)\n        chunksize = kwds.get(\"chunksize\", None)\n        if kwds.get(\"engine\") == \"pyarrow\":\n            if iterator:\n                raise ValueError(\n                    \"The 'iterator' option is not supported with the 'pyarrow' engine\"\n                )\n    \n            if chunksize is not None:\n                raise ValueError(\n                    \"The 'chunksize' option is not supported with the 'pyarrow' engine\"\n                )\n        else:\n            chunksize = validate_integer(\"chunksize\", kwds.get(\"chunksize\", None), 1)\n    \n        nrows = kwds.get(\"nrows\", None)\n    \n        # Check for duplicates in names.\n        _validate_names(kwds.get(\"names\", None))\n    \n        # Create the parser.\n>       parser = TextFileReader(filepath_or_buffer, **kwds)\n\n/opt/hostedtoolcache/Python/3.10.4/x64/lib/python3.10/site-packages/pandas/io/parsers/readers.py:575: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <pandas.io.parsers.readers.TextFileReader object at 0x7ff843cab640>\nf = '/home/runner/work/conduit/conduit/test_conduit/config/article_test_data.txt'\nengine = 'c'\nkwds = {'cache_dates': True, 'chunksize': None, 'comment': None, 'compression': 'infer', ...}\nengine_specified = True, dialect = None\noptions = {'cache_dates': True, 'comment': None, 'compression': 'infer', 'converters': None, ...}\n\n    def __init__(\n        self,\n        f: FilePath | ReadCsvBuffer[bytes] | ReadCsvBuffer[str] | list,\n        engine: CSVEngine | None = None,\n        **kwds,\n    ):\n        if engine is not None:\n            engine_specified = True\n        else:\n            engine = \"python\"\n            engine_specified = False\n        self.engine = engine\n        self._engine_specified = kwds.get(\"engine_specified\", engine_specified)\n    \n        _validate_skipfooter(kwds)\n    \n        dialect = _extract_dialect(kwds)\n        if dialect is not None:\n            if engine == \"pyarrow\":\n                raise ValueError(\n                    \"The 'dialect' option is not supported with the 'pyarrow' engine\"\n                )\n            kwds = _merge_with_dialect_properties(dialect, kwds)\n    \n        if kwds.get(\"header\", \"infer\") == \"infer\":\n            kwds[\"header\"] = 0 if kwds.get(\"names\") is None else None\n    \n        self.orig_options = kwds\n    \n        # miscellanea\n        self._currow = 0\n    \n        options = self._get_options_with_defaults(engine)\n        options[\"storage_options\"] = kwds.get(\"storage_options\", None)\n    \n        self.chunksize = options.pop(\"chunksize\", None)\n        self.nrows = options.pop(\"nrows\", None)\n    \n        self._check_file_or_buffer(f, engine)\n        self.options, self.engine = self._clean_options(options, engine)\n    \n        self.squeeze = self.options.pop(\"squeeze\", False)\n    \n        if \"has_index_names\" in kwds:\n            self.options[\"has_index_names\"] = kwds[\"has_index_names\"]\n    \n        self.handles: IOHandles | None = None\n>       self._engine = self._make_engine(f, self.engine)\n\n/opt/hostedtoolcache/Python/3.10.4/x64/lib/python3.10/site-packages/pandas/io/parsers/readers.py:933: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <pandas.io.parsers.readers.TextFileReader object at 0x7ff843cab640>\nf = '/home/runner/work/conduit/conduit/test_conduit/config/article_test_data.txt'\nengine = 'c'\n\n    def _make_engine(\n        self,\n        f: FilePath | ReadCsvBuffer[bytes] | ReadCsvBuffer[str] | list | IO,\n        engine: CSVEngine = \"c\",\n    ):\n        mapping: dict[str, type[ParserBase]] = {\n            \"c\": CParserWrapper,\n            \"python\": PythonParser,\n            \"pyarrow\": ArrowParserWrapper,\n            \"python-fwf\": FixedWidthFieldParser,\n        }\n        if engine not in mapping:\n            raise ValueError(\n                f\"Unknown engine: {engine} (valid options are {mapping.keys()})\"\n            )\n        if not isinstance(f, list):\n            # open file here\n            is_text = True\n            mode = \"r\"\n            if engine == \"pyarrow\":\n                is_text = False\n                mode = \"rb\"\n            # error: No overload variant of \"get_handle\" matches argument types\n            # \"Union[str, PathLike[str], ReadCsvBuffer[bytes], ReadCsvBuffer[str]]\"\n            # , \"str\", \"bool\", \"Any\", \"Any\", \"Any\", \"Any\", \"Any\"\n>           self.handles = get_handle(  # type: ignore[call-overload]\n                f,\n                mode,\n                encoding=self.options.get(\"encoding\", None),\n                compression=self.options.get(\"compression\", None),\n                memory_map=self.options.get(\"memory_map\", False),\n                is_text=is_text,\n                errors=self.options.get(\"encoding_errors\", \"strict\"),\n                storage_options=self.options.get(\"storage_options\", None),\n            )\n\n/opt/hostedtoolcache/Python/3.10.4/x64/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1217: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\npath_or_buf = '/home/runner/work/conduit/conduit/test_conduit/config/article_test_data.txt'\nmode = 'r'\n\n    @doc(compression_options=_shared_docs[\"compression_options\"] % \"path_or_buf\")\n    def get_handle(\n        path_or_buf: FilePath | BaseBuffer,\n        mode: str,\n        *,\n        encoding: str | None = None,\n        compression: CompressionOptions = None,\n        memory_map: bool = False,\n        is_text: bool = True,\n        errors: str | None = None,\n        storage_options: StorageOptions = None,\n    ) -> IOHandles[str] | IOHandles[bytes]:\n        \"\"\"\n        Get file handle for given path/buffer and mode.\n    \n        Parameters\n        ----------\n        path_or_buf : str or file handle\n            File path or object.\n        mode : str\n            Mode to open path_or_buf with.\n        encoding : str or None\n            Encoding to use.\n        {compression_options}\n    \n            .. versionchanged:: 1.0.0\n               May now be a dict with key 'method' as compression mode\n               and other keys as compression options if compression\n               mode is 'zip'.\n    \n            .. versionchanged:: 1.1.0\n               Passing compression options as keys in dict is now\n               supported for compression modes 'gzip', 'bz2', 'zstd' and 'zip'.\n    \n            .. versionchanged:: 1.4.0 Zstandard support.\n    \n        memory_map : bool, default False\n            See parsers._parser_params for more information.\n        is_text : bool, default True\n            Whether the type of the content passed to the file/buffer is string or\n            bytes. This is not the same as `\"b\" not in mode`. If a string content is\n            passed to a binary file/buffer, a wrapper is inserted.\n        errors : str, default 'strict'\n            Specifies how encoding and decoding errors are to be handled.\n            See the errors argument for :func:`open` for a full list\n            of options.\n        storage_options: StorageOptions = None\n            Passed to _get_filepath_or_buffer\n    \n        .. versionchanged:: 1.2.0\n    \n        Returns the dataclass IOHandles\n        \"\"\"\n        # Windows does not default to utf-8. Set to utf-8 for a consistent behavior\n        encoding = encoding or \"utf-8\"\n    \n        # read_csv does not know whether the buffer is opened in binary/text mode\n        if _is_binary_mode(path_or_buf, mode) and \"b\" not in mode:\n            mode += \"b\"\n    \n        # validate encoding and errors\n        codecs.lookup(encoding)\n        if isinstance(errors, str):\n            codecs.lookup_error(errors)\n    \n        # open URLs\n        ioargs = _get_filepath_or_buffer(\n            path_or_buf,\n            encoding=encoding,\n            compression=compression,\n            mode=mode,\n            storage_options=storage_options,\n        )\n    \n        handle = ioargs.filepath_or_buffer\n        handles: list[BaseBuffer]\n    \n        # memory mapping needs to be the first step\n        handle, memory_map, handles = _maybe_memory_map(\n            handle,\n            memory_map,\n            ioargs.encoding,\n            ioargs.mode,\n            errors,\n            ioargs.compression[\"method\"] not in _compression_to_extension,\n        )\n    \n        is_path = isinstance(handle, str)\n        compression_args = dict(ioargs.compression)\n        compression = compression_args.pop(\"method\")\n    \n        # Only for write methods\n        if \"r\" not in mode and is_path:\n            check_parent_directory(str(handle))\n    \n        if compression:\n            if compression != \"zstd\":\n                # compression libraries do not like an explicit text-mode\n                ioargs.mode = ioargs.mode.replace(\"t\", \"\")\n            elif compression == \"zstd\" and \"b\" not in ioargs.mode:\n                # python-zstandard defaults to text mode, but we always expect\n                # compression libraries to use binary mode.\n                ioargs.mode += \"b\"\n    \n            # GZ Compression\n            if compression == \"gzip\":\n                if is_path:\n                    assert isinstance(handle, str)\n                    # error: Incompatible types in assignment (expression has type\n                    # \"GzipFile\", variable has type \"Union[str, BaseBuffer]\")\n                    handle = gzip.GzipFile(  # type: ignore[assignment]\n                        filename=handle,\n                        mode=ioargs.mode,\n                        **compression_args,\n                    )\n                else:\n                    handle = gzip.GzipFile(\n                        # No overload variant of \"GzipFile\" matches argument types\n                        # \"Union[str, BaseBuffer]\", \"str\", \"Dict[str, Any]\"\n                        fileobj=handle,  # type: ignore[call-overload]\n                        mode=ioargs.mode,\n                        **compression_args,\n                    )\n    \n            # BZ Compression\n            elif compression == \"bz2\":\n                # No overload variant of \"BZ2File\" matches argument types\n                # \"Union[str, BaseBuffer]\", \"str\", \"Dict[str, Any]\"\n                handle = bz2.BZ2File(  # type: ignore[call-overload]\n                    handle,\n                    mode=ioargs.mode,\n                    **compression_args,\n                )\n    \n            # ZIP Compression\n            elif compression == \"zip\":\n                # error: Argument 1 to \"_BytesZipFile\" has incompatible type \"Union[str,\n                # BaseBuffer]\"; expected \"Union[Union[str, PathLike[str]],\n                # ReadBuffer[bytes], WriteBuffer[bytes]]\"\n                handle = _BytesZipFile(\n                    handle, ioargs.mode, **compression_args  # type: ignore[arg-type]\n                )\n                if handle.mode == \"r\":\n                    handles.append(handle)\n                    zip_names = handle.namelist()\n                    if len(zip_names) == 1:\n                        handle = handle.open(zip_names.pop())\n                    elif len(zip_names) == 0:\n                        raise ValueError(f\"Zero files found in ZIP file {path_or_buf}\")\n                    else:\n                        raise ValueError(\n                            \"Multiple files found in ZIP file. \"\n                            f\"Only one file per ZIP: {zip_names}\"\n                        )\n    \n            # XZ Compression\n            elif compression == \"xz\":\n                handle = get_lzma_file()(handle, ioargs.mode)\n    \n            # Zstd Compression\n            elif compression == \"zstd\":\n                zstd = import_optional_dependency(\"zstandard\")\n                if \"r\" in ioargs.mode:\n                    open_args = {\"dctx\": zstd.ZstdDecompressor(**compression_args)}\n                else:\n                    open_args = {\"cctx\": zstd.ZstdCompressor(**compression_args)}\n                handle = zstd.open(\n                    handle,\n                    mode=ioargs.mode,\n                    **open_args,\n                )\n    \n            # Unrecognized Compression\n            else:\n                msg = f\"Unrecognized compression type: {compression}\"\n                raise ValueError(msg)\n    \n            assert not isinstance(handle, str)\n            handles.append(handle)\n    \n        elif isinstance(handle, str):\n            # Check whether the filename is to be opened in binary mode.\n            # Binary mode does not support 'encoding' and 'newline'.\n            if ioargs.encoding and \"b\" not in ioargs.mode:\n                # Encoding\n>               handle = open(\n                    handle,\n                    ioargs.mode,\n                    encoding=ioargs.encoding,\n                    errors=errors,\n                    newline=\"\",\n                )\nE               FileNotFoundError: [Errno 2] No such file or directory: '/home/runner/work/conduit/conduit/test_conduit/config/article_test_data.txt'\n\n/opt/hostedtoolcache/Python/3.10.4/x64/lib/python3.10/site-packages/pandas/io/common.py:789: FileNotFoundError\n\nDuring handling of the above exception, another exception occurred:\n\nself = <test_conduit.tests.test_cases_11.TestConduit object at 0x7ff843caa230>\n\n    def setup(self):\n        try:\n            self.data_for_test = DataForTest()\n            self.browser = Browser().get_browser()\n            self.mp = MainPage(self.browser, self.data_for_test.BASE_URL, mode=Mode.TIMESLEEP)\n    \n        except BaseException:\n>           self.mp.log_error('Webdriver error.')\nE           AttributeError: 'TestConduit' object has no attribute 'mp'\n\ntest_conduit/tests/test_cases_11.py:20: AttributeError","flaky":false,"newFailed":false,"newBroken":false,"newPassed":false,"retriesCount":0,"retriesStatusChange":false,"beforeStages":[{"name":"_xunit_setup_method_fixture_TestConduit","time":{"start":1654520411064,"stop":1654520411064,"duration":0},"status":"broken","statusMessage":"AttributeError: 'TestConduit' object has no attribute 'mp'\n","statusTrace":"  File \"/opt/hostedtoolcache/Python/3.10.4/x64/lib/python3.10/site-packages/pluggy/_callers.py\", line 39, in _multicall\n    res = hook_impl.function(*args)\n  File \"/opt/hostedtoolcache/Python/3.10.4/x64/lib/python3.10/site-packages/_pytest/fixtures.py\", line 1111, in pytest_fixture_setup\n    result = call_fixture_func(fixturefunc, request, kwargs)\n  File \"/opt/hostedtoolcache/Python/3.10.4/x64/lib/python3.10/site-packages/_pytest/fixtures.py\", line 883, in call_fixture_func\n    fixture_result = next(generator)\n  File \"/opt/hostedtoolcache/Python/3.10.4/x64/lib/python3.10/site-packages/_pytest/python.py\", line 893, in xunit_setup_method_fixture\n    _call_with_optional_argument(func, method)\n  File \"/opt/hostedtoolcache/Python/3.10.4/x64/lib/python3.10/site-packages/_pytest/python.py\", line 778, in _call_with_optional_argument\n    func()\n  File \"/home/runner/work/conduit/conduit/test_conduit/tests/test_cases_11.py\", line 20, in setup\n    self.mp.log_error('Webdriver error.')\n","steps":[],"attachments":[],"parameters":[],"stepsCount":0,"attachmentsCount":0,"shouldDisplayMessage":true,"hasContent":true}],"afterStages":[],"labels":[{"name":"parentSuite","value":"test_conduit.tests"},{"name":"suite","value":"test_cases_11"},{"name":"subSuite","value":"TestConduit"},{"name":"host","value":"fv-az292-435"},{"name":"thread","value":"3012-MainThread"},{"name":"framework","value":"pytest"},{"name":"language","value":"cpython3"},{"name":"package","value":"test_conduit.tests.test_cases_11"},{"name":"resultFormat","value":"allure2"}],"parameters":[],"links":[],"hidden":false,"retry":false,"extra":{"severity":"normal","retries":[],"categories":[{"name":"Test defects","matchedStatuses":[],"flaky":false}],"tags":[]},"source":"f709452ef115bb9d.json","parameterValues":[]}